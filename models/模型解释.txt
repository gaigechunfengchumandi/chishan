'''
输入形状：torch.Size([32, 2500])  # 假设batch_size=32，sequence_length=2500

1. 输入展开维度后：
   -> torch.Size([32, 1, 2500])  # 添加通道维度

2. 经过3层卷积后：
   (每层卷积保持长度不变，通过padding=2实现)
   Conv1(32,32,kernel_size=5) -> [32, 32, 2500]
   Conv2(32,64,kernel_size=5) -> [32, 64, 2500]
   Conv3(64,128,kernel_size=5) -> [32, 128, 2500]

3. 维度变换后：
   -> torch.Size([32, 2500, 128])  # 准备输入LSTM

4. 双向LSTM处理：
   (hidden_size=64，双向输出128)
   -> torch.Size([32, 2500, 128])

5. 注意力机制处理后：
   -> 保持形状 [32, 2500, 128]

6. 全连接层处理后：
   fc1: 128 → 64
   fc2: 64 → 1
   -> torch.Size([32, 2500, 1])

最终输出形状：torch.Size([32, 2500, 1])
'''